#!/usr/bin/env python3
"""
Cycle OKE node pool workers by replacing their boot volumes.

The OCI documentation describes the behaviour where updating a node pool only
adjusts configuration for new nodes. Existing workers must be cycled (boot
volume replaced) to pick up the new image or Kubernetes version. This command
drives that workflow using the HTML report generated by ``oke_version_report``.
"""

from __future__ import annotations

import argparse
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

from rich.console import Console
from rich.logging import RichHandler

from oci import exceptions as oci_exceptions
from oci.container_engine.models import (
    NodeEvictionSettings,
    ReplaceBootVolumeClusterNodeDetails,
    UpdateNodePoolDetails,
)

from oci_client.models import OKEClusterInfo, OKENodePoolInfo
from oci_client.utils.display import display_warning
from oci_client.utils.session import create_oci_client, setup_session_token
from oke_upgrade import ReportCluster, load_clusters_from_report, _ReportHTMLParser

console = Console()
logger = logging.getLogger(__name__)

SKIP_NODE_STATES = {"DELETED", "DELETING"}


@dataclass
class NodeCycleResult:
    """Outcome for cycling a single node."""

    entry: ReportCluster
    node_pool_id: str
    node_pool_name: str
    node_id: str
    node_name: str
    status: str
    work_request_id: Optional[str]
    skipped: bool = False
    error: Optional[str] = None

    @property
    def success(self) -> bool:
        return (
            not self.skipped
            and self.error is None
            and self.status.upper() in {"SUCCEEDED", "IN_PROGRESS"}
        )


def _resolve_cluster_details(client: Any, cluster_id: str) -> OKEClusterInfo:
    """
    Retrieve cluster details either via the dedicated helper or by directly querying
    the Container Engine client (maintains compatibility with older client versions).
    """
    if hasattr(client, "get_oke_cluster"):
        return client.get_oke_cluster(cluster_id)  # type: ignore[attr-defined]

    ce_client = getattr(client, "container_engine_client", None)
    if ce_client is None:
        raise AttributeError("OCI client does not expose container_engine_client")

    cluster = ce_client.get_cluster(cluster_id).data
    available_upgrades_attr = getattr(cluster, "available_kubernetes_upgrades", None)
    if available_upgrades_attr is None:
        available_upgrades_attr = getattr(cluster, "available_upgrades", None)
    available_upgrades = list(available_upgrades_attr or [])

    return OKEClusterInfo(
        cluster_id=cluster_id,
        name=getattr(cluster, "name", cluster_id),
        kubernetes_version=getattr(cluster, "kubernetes_version", None),
        lifecycle_state=getattr(cluster, "lifecycle_state", None),
        compartment_id=getattr(cluster, "compartment_id", None),
        available_upgrades=available_upgrades,
    )


def _list_node_pools(client: Any, cluster_id: str, compartment_id: Optional[str]) -> List[OKENodePoolInfo]:
    """Retrieve node pools either via client helper or directly from the Container Engine API."""
    if hasattr(client, "list_node_pools"):
        return client.list_node_pools(cluster_id, compartment_id)  # type: ignore[attr-defined]

    ce_client = getattr(client, "container_engine_client", None)
    if ce_client is None:
        raise AttributeError("OCI client does not expose container_engine_client")

    request_kwargs: Dict[str, Any] = {"cluster_id": cluster_id}
    if compartment_id:
        request_kwargs["compartment_id"] = compartment_id

    response = ce_client.list_node_pools(**request_kwargs)
    data = getattr(response, "data", []) or []

    node_pools: List[OKENodePoolInfo] = []
    for pool in data:
        node_pool_id = getattr(pool, "id", None)
        if not node_pool_id:
            continue
        node_pools.append(
            OKENodePoolInfo(
                node_pool_id=node_pool_id,
                name=getattr(pool, "name", node_pool_id),
                kubernetes_version=getattr(pool, "kubernetes_version", None),
                lifecycle_state=getattr(pool, "lifecycle_state", None),
            )
        )
    return node_pools


def _fetch_node_pool_details(client: Any, node_pool_id: str) -> Any:
    ce_client = getattr(client, "container_engine_client", None)
    if ce_client is None:
        raise AttributeError("OCI client does not expose container_engine_client")
    return ce_client.get_node_pool(node_pool_id).data


def _extract_maximum_unavailable(node_pool_details: Any) -> int:
    details = getattr(node_pool_details, "node_pool_cycling_details", None)
    candidate = getattr(details, "maximum_unavailable", None) if details is not None else None
    if candidate in (None, "", 0):
        return 3
    try:
        value = int(candidate)
    except (TypeError, ValueError):
        logger.debug("Unable to parse maximum_unavailable=%r; defaulting to 3", candidate)
        return 3
    return max(value, 1)


def _normalize_version(value: Optional[str]) -> Optional[str]:
    if not value:
        return None
    stripped = value.strip()
    if stripped.startswith("v"):
        stripped = stripped[1:]
    return stripped or None


def _ensure_node_pool_version(
    client: Any,
    node_pool_id: str,
    current_version: Optional[str],
    desired_version: Optional[str],
) -> Optional[str]:
    normalized_current = _normalize_version(current_version)
    normalized_desired = _normalize_version(desired_version)
    if not normalized_desired:
        return current_version
    if normalized_current == normalized_desired:
        return current_version

    ce_client = getattr(client, "container_engine_client", None)
    if ce_client is None:
        logger.warning("Cannot align node pool version because container_engine_client is missing")
        return current_version

    logger.info(
        "Aligning node pool %s to Kubernetes version %s (was %s)",
        node_pool_id,
        normalized_desired,
        normalized_current,
    )
    try:
        ce_client.update_node_pool(
            node_pool_id,
            UpdateNodePoolDetails(kubernetes_version=desired_version),
        )
    except oci_exceptions.ServiceError as exc:
        logger.error(
            "Failed to align node pool %s to version %s: %s",
            node_pool_id,
            desired_version,
            exc.message,
        )
        return current_version
    return desired_version




def _cycle_node(
    entry: ReportCluster,
    node_pool: OKENodePoolInfo,
    node: Any,
    client: Any,
    *,
    grace_period: str,
    force_after_grace: bool,
    dry_run: bool,
) -> NodeCycleResult:
    node_id = getattr(node, "id", None)
    node_name = getattr(node, "name", node_id or "unknown")
    lifecycle_state = (getattr(node, "lifecycle_state", None) or "").upper()

    if not node_id:
        message = f"Node pool {node_pool.name} contains a node without an OCID; skipping."
        display_warning(message)
        return NodeCycleResult(
            entry=entry,
            node_pool_id=node_pool.node_pool_id,
            node_pool_name=node_pool.name,
            node_id="N/A",
            node_name=node_name,
            status="SKIPPED",
            work_request_id=None,
            skipped=True,
            error=message,
        )

    if lifecycle_state in SKIP_NODE_STATES:
        logger.info(
            "Skipping node %s (%s) in pool %s because lifecycle_state=%s",
            node_name,
            node_id,
            node_pool.node_pool_id,
            lifecycle_state,
        )
        return NodeCycleResult(
            entry=entry,
            node_pool_id=node_pool.node_pool_id,
            node_pool_name=node_pool.name,
            node_id=node_id,
            node_name=node_name,
            status=lifecycle_state or "SKIPPED",
            work_request_id=None,
            skipped=True,
        )

    if dry_run:
        logger.info(
            "[DRY RUN] Would replace boot volume for node %s (%s) in pool %s (%s) within cluster %s.",
            node_name,
            node_id,
            node_pool.name,
            node_pool.node_pool_id,
            entry.cluster_name,
        )
        return NodeCycleResult(
            entry=entry,
            node_pool_id=node_pool.node_pool_id,
            node_pool_name=node_pool.name,
            node_id=node_id,
            node_name=node_name,
            status="DRY_RUN",
            work_request_id=None,
            skipped=True,
        )

    ce_client = getattr(client, "container_engine_client", None)
    if ce_client is None:
        message = "OCI client does not expose container_engine_client"
        logger.error(message)
        return NodeCycleResult(
            entry=entry,
            node_pool_id=node_pool.node_pool_id,
            node_pool_name=node_pool.name,
            node_id=node_id,
            node_name=node_name,
            status="FAILED",
            work_request_id=None,
            error=message,
        )

    eviction_settings = NodeEvictionSettings(
        eviction_grace_duration=grace_period,
        is_force_action_after_grace_duration=force_after_grace,
    )
    details = ReplaceBootVolumeClusterNodeDetails(node_eviction_settings=eviction_settings)

    logger.info(
        "Replacing boot volume for node %s (%s) in pool %s (%s) within cluster %s (%s).",
        node_name,
        node_id,
        node_pool.name,
        node_pool.node_pool_id,
        entry.cluster_name,
        entry.cluster_ocid,
    )

    try:
        response = ce_client.replace_boot_volume_cluster_node(
            entry.cluster_ocid,
            node_id,
            details,
        )
    except oci_exceptions.ServiceError as exc:
        logger.error(
            "Failed to replace boot volume for node %s (%s) in pool %s: %s",
            node_name,
            node_id,
            node_pool.node_pool_id,
            exc.message,
        )
        return NodeCycleResult(
            entry=entry,
            node_pool_id=node_pool.node_pool_id,
            node_pool_name=node_pool.name,
            node_id=node_id,
            node_name=node_name,
            status="FAILED",
            work_request_id=None,
            error=exc.message,
        )

    work_request_id = response.headers.get("opc-work-request-id")
    return NodeCycleResult(
        entry=entry,
        node_pool_id=node_pool.node_pool_id,
        node_pool_name=node_pool.name,
        node_id=node_id,
        node_name=node_name,
        status="IN_PROGRESS" if work_request_id else "UNKNOWN",
        work_request_id=work_request_id,
    )


def perform_node_cycles(
    entries: Sequence[ReportCluster],
    *,
    grace_period: str,
    force_after_grace: bool,
    dry_run: bool,
) -> List[NodeCycleResult]:
    results: List[NodeCycleResult] = []
    clients: Dict[Tuple[str, str, str], Any] = {}

    for entry in entries:
        client_key = (entry.project, entry.stage, entry.region)
        if client_key not in clients:
            profile_name = setup_session_token(entry.project, entry.stage, entry.region)
            client = create_oci_client(entry.region, profile_name)
            if not client:
                message = (
                    f"Failed to initialize OCI client for {entry.project}/{entry.stage} in {entry.region}."
                )
                display_warning(message)
                results.append(
                    NodeCycleResult(
                        entry=entry,
                        node_pool_id="N/A",
                        node_pool_name="N/A",
                        node_id="N/A",
                        node_name="N/A",
                        status="FAILED",
                        work_request_id=None,
                        error=message,
                    )
                )
                continue
            clients[client_key] = client

        client = clients[client_key]
        try:
            cluster_info = _resolve_cluster_details(client, entry.cluster_ocid)
        except Exception as exc:  # pragma: no cover - defensive guard
            message = (
                f"Failed to resolve cluster details for {entry.cluster_name} "
                f"({entry.cluster_ocid}) in {entry.region}: {exc}"
            )
            display_warning(message)
            results.append(
                NodeCycleResult(
                    entry=entry,
                    node_pool_id="N/A",
                    node_pool_name="N/A",
                    node_id="N/A",
                    node_name="N/A",
                    status="FAILED",
                    work_request_id=None,
                    error=str(exc),
                )
            )
            continue

        if cluster_info.available_upgrades:
            display_warning(
                f"Cluster {entry.cluster_name} ({entry.cluster_ocid}) still lists available control "
                "plane upgrades. Complete the control plane upgrade and regenerate the report before cycling nodes."
            )

        try:
            node_pools = _list_node_pools(client, entry.cluster_ocid, cluster_info.compartment_id)
        except Exception as exc:  # pragma: no cover - defensive guard
            message = (
                f"Failed to list node pools for cluster {entry.cluster_name} "
                f"({entry.cluster_ocid}): {exc}"
            )
            display_warning(message)
            results.append(
                NodeCycleResult(
                    entry=entry,
                    node_pool_id="N/A",
                    node_pool_name="N/A",
                    node_id="N/A",
                    node_name="N/A",
                    status="FAILED",
                    work_request_id=None,
                    error=str(exc),
                )
            )
            continue

        for node_pool in node_pools:
            try:
                node_pool_details = _fetch_node_pool_details(client, node_pool.node_pool_id)
            except Exception as exc:  # pragma: no cover - defensive guard
                message = (
                    f"Failed to fetch node pool {node_pool.name} ({node_pool.node_pool_id}) details: {exc}"
                )
                display_warning(message)
                results.append(
                    NodeCycleResult(
                        entry=entry,
                        node_pool_id=node_pool.node_pool_id,
                        node_pool_name=node_pool.name,
                        node_id="N/A",
                        node_name="N/A",
                        status="FAILED",
                        work_request_id=None,
                        error=str(exc),
                    )
                )
                continue

            pool_version = getattr(node_pool_details, "kubernetes_version", None)
            desired_version = _ensure_node_pool_version(
                client,
                node_pool.node_pool_id,
                pool_version,
                cluster_info.kubernetes_version,
            )

            nodes = getattr(node_pool_details, "nodes", None) or []
            maximum_unavailable = _extract_maximum_unavailable(node_pool_details)
            logger.info(
                "Node pool %s (%s) maximum_unavailable=%s target_version=%s",
                node_pool.name,
                node_pool.node_pool_id,
                maximum_unavailable,
                desired_version,
            )
            if not nodes:
                console.print(
                    f"[dim]Node pool [cyan]{node_pool.name}[/cyan] "
                    f"({node_pool.node_pool_id}) has no worker nodes; skipping.[/dim]"
                )
                continue

            for node in nodes:
                node_version = getattr(node, "kubernetes_version", None)
                if desired_version and _normalize_version(node_version) == _normalize_version(desired_version):
                    logger.debug(
                        "Skipping node %s (%s) because it already reports Kubernetes version %s",
                        getattr(node, "name", node_version),
                        getattr(node, "id", "unknown"),
                        node_version,
                    )
                    continue

                result = _cycle_node(
                    entry,
                    node_pool,
                    node,
                    client,
                    grace_period=grace_period,
                    force_after_grace=force_after_grace,
                    dry_run=dry_run,
                )
                results.append(result)

    return results


def _summarize(results: Iterable[NodeCycleResult]) -> Tuple[int, int, int]:
    initiated = sum(1 for item in results if item.success)
    failures = sum(1 for item in results if not item.skipped and not item.success)
    skipped = sum(1 for item in results if item.skipped)
    return initiated, skipped, failures


def _diagnose_report(report_path: Path) -> List[str]:
    """Provide context when the HTML report does not produce any cluster entries."""
    diagnostics: List[str] = []
    try:
        parser = _ReportHTMLParser()
        html_content = report_path.read_text(encoding="utf-8")
        parser.feed(html_content)
    except Exception as exc:  # pragma: no cover - defensive
        return [f"Failed to parse report {report_path}: {exc}"]

    total_rows = len(parser.rows)
    if total_rows == 0:
        diagnostics.append(
            "Report parser did not find any <tbody> rows. Ensure the report was generated with the latest tooling."
        )
        return diagnostics

    diagnostics.append(f"Report parser captured {total_rows} row(s).")

    short_rows: List[Tuple[int, int, List[str]]] = []
    for idx, row in enumerate(parser.rows, start=1):
        if len(row) < 9:
            short_rows.append((idx, len(row), row))

    if short_rows:
        diagnostics.append(
            "The following row(s) contain fewer than 9 columns (project/stage/region/... columns are required):"
        )
        for idx, length, row in short_rows[:5]:
            sample = ", ".join(row[: min(len(row), 4)])
            diagnostics.append(f"  Row {idx}: {length} column(s). Sample data: {sample}")
        if len(short_rows) > 5:
            diagnostics.append(f"  â€¦ {len(short_rows) - 5} additional row(s) omitted.")
    else:
        diagnostics.append("All parsed rows have the expected number of columns.")

    return diagnostics


def configure_logging(verbose: bool = False) -> None:
    logging.basicConfig(
        level=logging.DEBUG if verbose else logging.INFO,
        format="%(message)s",
        handlers=[RichHandler(rich_tracebacks=True)],
    )


def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Cycle OKE node pools to replace worker boot volumes based on an HTML report.",
    )
    parser.add_argument("report_path", help="Path to the HTML report generated by oke_version_report.")
    parser.add_argument(
        "--grace-period",
        default="PT30M",
        help="Eviction grace period in ISO-8601 duration format (default: PT30M).",
    )
    parser.add_argument(
        "--force-after-grace",
        action="store_true",
        help="Force deletion after the grace period elapses.",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show planned node cycles without calling OCI APIs.",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable debug logging.",
    )
    return parser.parse_args()


def main() -> int:
    args = parse_arguments()
    configure_logging(verbose=args.verbose)

    report_path = Path(args.report_path).expanduser().resolve()
    if not report_path.exists():
        console.print(f"[bold red]âœ— Report not found:[/bold red] {report_path}")
        return 1

    entries = load_clusters_from_report(report_path)
    if not entries:
        console.print("[bold red]âœ— No clusters found in the provided report.[/bold red]")
        for line in _diagnose_report(report_path):
            console.print(f"[dim]- {line}[/dim]")
        return 1

    console.print("ðŸš€ Initiating OKE node cycling workflow...")
    console.print(f"ðŸ“„ Using report: {report_path}")

    results = perform_node_cycles(
        entries,
        grace_period=args.grace_period,
        force_after_grace=args.force_after_grace,
        dry_run=args.dry_run,
    )

    initiated, skipped, failures = _summarize(results)
    console.print(
        f"Summary: initiated {initiated} cycle(s); {skipped} skipped; {failures} failure(s)."
    )

    return 0 if failures == 0 else 1


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
